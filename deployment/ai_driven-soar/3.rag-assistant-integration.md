<h1 align="center">
SOCIMP TheHive RAG Assistant
</h1>

![Knowledgebase-Enrichment](/images/n8n/SOCIMP-RAG-Chatbot.svg)

<div align="center"> 
  SOCIMP TheHive RAG Assistant
</div>


## The Challenge

In the previous workflow - [Knowledgebase Enrichment](/deployment/ai_driven-soar/2.knowledgebase-enichment.md), I described the idea how to embed a Chatbot (AI Assistant) into TheHive to support analysts during investigations.

While embedding a generic chatbot is simple, creating one with internal knowledge and environmental context is more challenging. Retrieval-Augmented Generation (RAG) addresses this by connecting the LLM to an external database, avoiding the need for costly and complex model retraining.

This workflow extends the previous one and puts the concept into practical use.

## The Idea

By leveraging TheHive's API, especially the comment-related endpoints, I was able to build a chatbot-like assistant directly inside TheHive. Although TheHive does not natively support chatbot integration, its rich documentation made it possible to implement this functionality creatively.

## About the Workflow Itself

The workflow is straightforward and easy to follow:

- A **Webhook Trigger** listens for a new comment on any case in TheHive 5.
- An **IF node** checks whether the comment starts with a specific prefix. If it does:
  - The assistant proceeds to parse the analyst’s input.
  - The message is passed to the **AI Agent**, which performs a semantic search on the internal knowledgebase to generate a context-aware response.
  - The case ID is used as a session identifier to maintain chat context across messages.

If the comment doesn't contain the correct prefix, the workflow exits without affecting regular analyst discussions in the case comment panel.

- The **AI Agent** responds with relevant context-enriched information.
- A **TheHive node** posts the AI’s response back to the case comment thread, providing analysts with intelligent, helpful feedback to assist their investigation.

## Workflow in Details

### Stage 1: Receive TheHive comment and check prefix

<div align="center">
  <img src="/images/n8n/pic14.png" alt="pic14" />
</div>

The first stage of this workflow begins with a **TheHive Trigger** node, which activates the workflow when specific events occur, in this case, when **a comment is created in a case**.

To configure this trigger, you can refer to the [official n8n documentation for the TheHive5 Trigger Node](https://docs.n8n.io/integrations/builtin/trigger-nodes/n8n-nodes-base.thehive5trigger/).

If you prefer using TheHive’s web interface, refer to [StrangeBee’s documentation on webhook notifier setup](https://docs.strangebee.com/thehive/user-guides/organization/configure-organization/manage-notifications/notifiers/webhook/).


**TheHive Trigger Node Configuration**:
- **Events**: Comment Created

Example Ouput:
```json
[
  {
    "event": "comment_create",
    "body": {
      "_id": "~295144",
      "_type": "Audit",
      "_createdBy": "example@gmail.com",
      "_createdAt": 1750825954042,
      "action": "create",
      "requestId": "1d4ff05956289640:-681688cd:197967a2e1f:-8000:3910",
      "rootId": "~123125968",
      "details": {
        "author": "example@gmail.com"
      },
      "objectId": "~123138256",
      "objectType": "Comment",
      "object": {
        "message": "@ai check knowledge base, i want more information about username susan.newman",
        "case": {
          "_id": "~123125968",
          "_type": "Case",
          "_createdBy": "example@gmail.com",
          "_updatedBy": "example@gmail.com",
          "_createdAt": 1750825799019,
          "_updatedAt": 1750825953686,
          "number": 132,
          "title": "Process Termination followed by Deletion [Cloned For Test]",
          "description": "### Alert Description\n- **Source**: SOCIMP Elastic Security\n- **Infected Host-Machine**: `SOCIMP-W-USER2`\n- **Infected Username**: `socimp-w-user2.socimp.local`\n- **Infected IP Address**: `192.168.168.4,127.0.0.1,::1`\n- **Severity**: medium\n- **Risk Score**: `47`\n- **Details**: Identifies a process termination event quickly followed by the deletion of its executable file. Malware tools and other non-native files dropped or created on a system by an adversary may leave traces to indicate to what occurred. Removal of these files can occur during an intrusion, or as part of a post-intrusion process to minimize the adversary's footprint.\n- **References**: \n",
          "severity": 2,
          "severityLabel": "MEDIUM",
          "startDate": 1750825799011,
          "tags": [
            "Data Source: Elastic Endgame",
            "Resources: Investigation Guide",
            "OS: Windows",
            "Data Source: Elastic Defend",
            "Tactic: Defense Evasion",
            "Domain: Endpoint",
            "Use Case: Threat Detection"
```

This payload contains useful metadata such as:
- The comment message content
- The case ID and title
- The user who posted the comment
...

To determine whether the comment should trigger the AI agent, I use an **IF Node** to evaluate the comment message. The logic checks for the presence of a specific prefix: `@ai` or `@AI` .

IF Node Configuration:
- Conditions:
    {{ $json.body.object.message }} contains "@AI" OR {{ $json.body.object.message }} contains "@ai"

If the condition evaluates to `true`, the workflow proceeds to **Stage 2**, where the comment message is parsed and processed by the AI agent.

If the condition is false, the workflow terminates or routes to a generic **error/stop node**, preventing unnecessary processing of unrelated comments.

### Stage 2: Extract Message

<div align="center">
  <img src="/images/n8n/pic15.png" alt="pic15" />
</div>

In this stage, the workflow extracts the comment message from the previous node using the **Edit Node** in n8n. Additionally, a **Stop and Error** node is used to gracefully handle cases where the comment does not follow the correct trigger format.

**`Edit Node` Configuration**:
- **Mode**: Manual Mapping
- **Fields to Set**
    - body.object.message: {{ $json.body.object.message }} (String)

Example Output:
```json
[
  {
    "body": {
      "object": {
        "message": "@ai check knowledge base, i want more information about username susan.newman"
      }
    }
  }
]

#This output isolates the comment text, making it easier for downstream nodes to process the AI query.
```

**`Stop and Error` Configuration**:
- **Error Type**: Error Message
- **Error Message**: Valid syntax is @AI or @ai

>**Important**: Make sure to set **"On Error"** → Continue in this node’s settings.
>
>This ensures the workflow won’t stop entirely if a comment does **not include** the expected AI prefix. Instead, it quietly skips unrelated messages while allowing valid requests to continue.

### Stage 3: RAG AI Context enrichment (Internal Knowledegebase)

<div align="center">
  <img src="/images/n8n/pic16.png" alt="pic16" />
</div>

This is the most critical stage of the workflow. By integrating an **AI Agent** in n8n and connecting it to a vector database that stores internal knowledge as embeddings, we can build a chatbot that is not only intelligent but context-aware, capable of understanding and responding based on the SOCIMP environment.

The AI Agent takes the extracted message from the previous stage, searches the **knowledge base** if the message references **internal context** (such as usernames, hosts, or configurations), and then generates a response. If the query is general, the agent responds like a typical LLM; if it's related to internal data, it uses the vector database for enrichment.

**`AI Agent` Node Configuration**:
- **Source for Prompt (User Message)**: Define below
    - **Prompt (User Message)**: {{ $json.body.object.message }}
- **Options**
    - **System Message**:

```text
Always call the internal document search tool first, even if the question seems general.
You're a SOC AI Assistant.
Respond clearly and accurately using plain text only, no markdown or formatting characters.
Keep answers concise and actionable.
When possible, link to known TTPs and suggest relevant investigative steps.
If a question refers to unknown internal data, simply respond with “I don't know.”
```

Required Inputs for AI Agent:
**1. Chat Model**
The model used for inference. In my project, I use Google Gemini 2.0, which is fast, concise, and free under limited usage.

**2. Memory (Optional)**
Used to maintain conversation history or session context. I use Simple Memory (built into n8n).

**3. Tool (Optional but Must-have in our workflow)**
This connects the AI Agent with external data sources-in this case, Pinecone, which holds our vectorized knowledgebase.

**`Memory` Input Configuration:**
- **Session ID**: Define below
    - **Key**: {{ $('Receive Comment').item.json.body.object.case._id }}
- **Context Window Length**: 5 (Max 5 interactions receives as context for output)

**`Pinecone Vector Store` Input Configuration**:
- **Credential to connect with**: Pinecone API account (which i mentioned in the previous workflow how to create one)
- **Operation Mode**: Retrieve Documents (As Tool for AI Agent)
- **Description**: This is important since it interact directly to AI Agent node, better prompt = better context output, im was pain in ass when dealing with how to prompt it properly to make the AI Agent really use Pinecone as external database.
```text
***Note: If the tool description is not clear, the AI model may ignore it and respond with "I don't know."***

A tool to search internal SOCIMP documentation and knowledge base.
Always use this tool if the user's message requires any organizational knowledge.
```
- **Pinecone Index**
    - **From list**: socimp-n8n-index
- **Limit**: 50
- **Include Metadata**: True
- **Options**:
    - **Pinecone Namespace**: SOCIMP-n8n

**Example Output (when configured correctly)**:
```json
[
  {
    "output": "sAMAccountName: Susan.Newman\npassword: socimp123!\nou: OU=Operations,OU=SOCIMP Users,DC=socimp,DC=local\nDepartment: Operations\nUserPrincipalName: Susan.Newman@socimp.com\nGivenName: Susan\ninitials: S\nSurname: Newman\nsn: Susan Newman\nDisplayname: Susan Newman\nOffice: Operations Office\nOfficePhone: 845-747-9580\nEmail: Susan.Newman@socimp.com\nStreetAddress: 3098 Old Dear Lane\nCity: Poughkeepsie\nState: HN\nPostalCode: 10000\nCountry: VN\nTitle: Operations Staff\nCompany: SOC in my Pocket"
  }
]
```
> This level of contextual response isn’t possible in standard chatbots. By integrating vectorized internal documentation and user data into the AI pipeline, we create a powerful assistant that understands our environment and provides relevant, actionable intelligence on demand.

### Stage 4: Response in TheHive's comment panel like Chatbot assistant

<div> 
    <img src="/images/n8n/pic17.png" alt="pic17" />
</div>

In the final step of the workflow, the AI-generated output is posted back into **TheHive** as a comment on the case that triggered the request. This effectively simulates a chatbot assistant replying to the SOC analyst directly within their investigation workspace.

This is done using **TheHive** `add : comment` node.

**TheHive `add : comment` Node Configuration**:
- **Credential to connect with**: TheHive API Key
- **Resource**: Comment
- **Operation**: Create
- **Add to**: Case
- **By ID**: {{ $('Receive Comment').item.json.body.object.case._id }}
- **Message**: {{ $json.output }}

**Example Output**:
```json
[
  {
    "_id": "~82174104",
    "_type": "Comment",
    "createdBy": "socimpai@soimp.local",
    "createdAt": 1750825961687,
    "message": "sAMAccountName: Susan.Newman\npassword: socimp123!\nou: OU=Operations,OU=SOCIMP Users,DC=socimp,DC=local\nDepartment: Operations\nUserPrincipalName: Susan.Newman@socimp.com\nGivenName: Susan\ninitials: S\nSurname: Newman\nsn: Susan Newman\nDisplayname: Susan Newman\nOffice: Operations Office\nOfficePhone: 845-747-9580\nEmail: Susan.Newman@socimp.com\nStreetAddress: 3098 Old Dear Lane\nCity: Poughkeepsie\nState: HN\nPostalCode: 10000\nCountry: VN\nTitle: Operations Staff\nCompany: SOC in my Pocket",
    "isEdited": false,
    "extraData": {}
  }
]
```

## Workflow in Action
<div> 
    <img src="/images/n8n/pic18.png" alt="pic18" />
</div>

Let’s walk through a real example from [Workflow #1](/deployment/ai_driven-soar/1.workflow#1-case-ingestion.md)

![pic19](/images/n8n/pic19.png)

In this incident, the case involves:
- Affected user: Susan Newman
- Suspected file: PhamThanhSang-SOC-L1.exe

This file name could indicate a malicious CV lure, often used in phishing or social engineering campaigns. Naturally, an analyst might want more information about both **the user** and the **suspicious file**.

To investigate the user, simply type the something like below into the comment panel:

```txt
@ai check knowledgebase, give me information about Susan Newman
```

![pic20](/images/n8n/pic20.png)

The AI Assistant will respond with all available internal information related to Susan Newman, retrieved from knowledge base.

Now, let’s check if the filename PhamThanhSang-SOC-L1.exe has been seen before in the environment.

```txt
@AI check knowledge, is there any information related to the file PhamThanhSang-SOC-L1.exe
```

![pic21](/images/n8n/pic21.png)

<div> 
    <img src="/images/n8n/pic22.png" alt="pic22" />
</div>

In this example, the AI is able to identify that this file is part of a simulated phishing campaign that was previously documented and ingested into the knowledge base as threat intelligence.

## Future Improvement

While this workflow has been working great so far and has proven really helpful during the initial implementation, I see a few areas where it could be improved-especially if I want to scale it up for a real production environment:

- **Performance & Fault Tolerance**  
  In a real SOC setup, the system have to be more resilient. If something fails-like a response from the AI, a Pinecone query, or a part of the workflow-I want it to recover gracefully, maybe by retrying or sending an alert instead of just stopping.

- **Speed & Latency**  
  The current response time is acceptable, but I’d like it to be faster, especially to keep the experience smooth for analysts. I might look into things like:
  - Preloading embeddings for common entities.
  - Caching frequent queries.
  - Optimizing how I split and embed text.

- **Security & Access Control**  
  Right now, the AI has access to everything in the vector store. I’d like to introduce some form of access control later-like limiting which data different users or teams can query-to make sure sensitive info stays protected.

- **Dynamic Knowledgebase Updates**  
  At the moment, I’m uploading files manually to keep the knowledgebase updated. In the future, I want to:
  - Pull data automatically from systems like Active Directory or the SIEM.
  - Ingest IOCs or threat reports from sources like MISP, OpenCTI.
  - Regularly reprocess the knowledgebase to keep everything current.

