<h1 align="center">

SOCIMP Knowledgebase Enrichment 
</h1>

![Knowledgebase-Enrichment](/images/n8n/Knowledgebase-Enrichment.svg)

<div align="center"> 
  Semi-automated Knowledgebase Enrichment
</div>

## The Challenge

If you’ve seen [my very first SOAR workflow](/deployment/ai_driven-soar/1.workflow#1-case-ingestion.md), we successfully generated AI-powered messages and posted them into TheHive’s comment panel. That moment felt like having our own version of ChatGPT integrated directly into our case management platform.

This achievement led to a new idea: building a workflow, and eventually a chatbot within TheHive to help SOC analysts investigate cases more efficiently and effectively.

However, while creating a simple chatbot is relatively easy, embedding one inside TheHive with awareness of internal environment data is much more complex.

## The Idea

While reading TheHive5 API documentation, I discovered that we can configure webhooks to trigger on various events (e.g., Case Created, Deleted, Merged, Alert Created, Deleted, etc.).

This opened up a new possibility: using a webhook that triggers when a comment is added to a case, allowing us to create an AI chatbot that reacts to analyst inputs.

But before jumping ahead to chatbot development, I realized a prerequisite was needed—a proper knowledge base. This led me into deeper research.

That’s where **RAG - Retrieval-Augmented Generation** comes in to solve the problem.

According [Amazon Web Service](https://aws.amazon.com/what-is/retrieval-augmented-generation/)

>"**Retrieval-Augmented Generation (RAG)** is the process of optimizing the output of a large language model, so it references an authoritative knowledge base outside of its training data sources before generating a response. Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. RAG extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base, all without the need to retrain the model. It is a cost-effective approach to improving LLM output so it remains relevant, accurate, and useful in various contexts."

Another key concept from AWS is how to build the "knowledge base"

>**Create external data**
>
>The new data outside of the LLM's original training data set is called external data. It can come from multiple data sources, such as a APIs, databases, or document repositories. The data may exist in various formats like files, database records, or long-form text. Another AI technique, called embedding language models, converts data into numerical representations and stores it in a vector database. This process creates a knowledge library that the generative AI models can understand.

## About the Workflow Itself

By taking advantages of **embedding language models**, I designed a simple workflow to store internal knowledge in a vector database.

To manage SOCIMP project-related information (e.g., network diagrams, subnet details, host naming conventions), I chose Google Drive as the document repository. It’s cloud-based, easy to use, and free—perfect for this use case.

Whenever a file (document, image, CSV, JSON, etc.) is added or updated in a specific Google Drive folder (The "Knowledgebase" folder), the workflow is triggered via the [Google Drive API](https://developers.google.com/workspace/drive/api/guides/about-sdk).

After retrieving metadata such as the file ID and filename, the workflow downloads the file and extracts its contents. These contents are then embedded and stored in a vector database.

For the vector database, I chose [Pinecone](https://www.pinecone.io/). It’s free within usage limits, and its performance is more than sufficient for this project.

Pinecone node will handle the jobs for us:
- Chunking the document into manageable sections
- Converting those chunks into high-dimensional vectors (embeddings) 
- And storing everything efficiently—ready for real-time retrieval when an internal knowledge query is made.

## Workflow in Details

### Stage 1: Retrieve Document Metadata

<div align="center">
  <img src="/images/n8n/pic7.png" alt="pic7" />
</div>

The first step in this workflow handles metadata from files uploaded to the **Knowledgebase** folder. Fortunately, n8n comes with a built-in Google Drive integration that simplifies this process.

In this stage, I use two Google Drive nodes:
- **`fileUpdated`**: Watches for files that are updated within the folder.
- **`fileCreated`**: Watches for new files uploaded into the folder.

**`fileUpdated` Node Configuration**:
- **Credential to connect with**: Google Drive account (You can find setup instructions in the Google Cloud Platform Console documentation.)
- **Poll Times**
  - **Mode**: Every Minute
- **Trigger On**: Changes Involving a Specific Folder
- **Folder**
  - **From list**: SOCIMP-n8n (Which is folder to store SOCIMP knowledgebase documents)
- **Watch For**: File Updated

**Example Output**:
```json
[
...
    "hasThumbnail": false,
    "thumbnailVersion": "0",
    "viewedByMe": true,
    "viewedByMeTime": "2025-06-23T16:36:39.789Z",
    "createdTime": "2025-06-23T16:36:39.789Z",
    "modifiedTime": "2025-06-23T10:04:42.000Z",
    "modifiedByMeTime": "2025-06-23T10:04:42.000Z",
    "modifiedByMe": true,
    "shared": false,
    "ownedByMe": true,
    "viewersCanCopyContent": true,
    "copyRequiresWriterPermission": false,
    "writersCanShare": true,
    "originalFilename": "socimp-active-directory-organization-units.csv",
    "fullFileExtension": "csv",
    "fileExtension": "csv",
    "md5Checksum": "792a1ac48f1053aed13d54667f56f895",
    "sha1Checksum": "e9effa509d9bb8f669b7184a702fde823c44a436",
    "sha256Checksum": "97ea7d0446c04cde9799e6667f6319a6d2897e55ce7bece7079c128ffd731454",
    "size": "1236",
    "quotaBytesUsed": "1236",
    "headRevisionId": "0B01jsrdywEJPUG9jVllDdGRTeSs1bmsxaDArd0svSnQrMHlzPQ",
    "isAppAuthorized": false,
    "inheritedPermissionsDisabled": false
  }
]
```

The configuration of the `fileCreated` node is nearly identical to fileUpdated, with one key difference:
- **Watch For**: File Created

### Stage 2: Downloaded file to inspect its contents

<div align="center">
  <img src="/images/n8n/pic10.png" alt="pic10" />
</div>

In this stage, we load the actual contents of the file so it can later be embedded into the vector database.

The **Google Drive** `download: file` node solves this task by using the file ID obtained from the previous metadata extraction step.

**`download: file` Node Configuration**:
- **Credential to connect with**: Same as previous node
- **Resource**: File
- **Operation**: Download
- **File**
  - **By ID**: {{ $json.id }}

The output of this node will be the binary content of the selected file, ready for processing, the binary file will be passed on to the next step for parsing, splitting, and embedding into the vector database.:

```text
data
------
File Name:
socimp-active-directory-organization-units.csv
File Extension:
csv
Mime Type:
text/csv
File Size:
1.24 kB
```

### Stage 3: Vectorization and Storage

<div align="center">
  <img src="/images/n8n/pic11.png" alt="pic11" />
</div>

This final stage is the most important which is embedding the document content into a vector database for future retrieval and AI enrichment.

n8n offers excellent AI integration tools that make this process easier to implement.

>Even though platforms like n8n simplify AI workflow creation, it's still critical to understand the concepts and technologies behind the scenes to build reliable, scalable solutions.

The `Pinecone Vector Store` node requires two key inputs:
- **Document**: The content to embed, which will be split into smaller chunks (important for processing large documents efficiently).
- **Embedding**: The model used to convert each chunk into a high-dimensional vector (e.g., OpenAI, Google Gemini, Azure, AWS, etc.).

Since we downloaded the file from **Google Drive**, I used the Default Data Loader node to parse the document and prepare it for embedding.

**`Default Data Loader` Node Configuration**:
- **Type of Data**: Binary
- **Mode**: Load All Input Data
- **Data Format**: Automatically Detect by Mine Type
- **Text Splitting**: Custom
- **Options**
  - **Metadata**
    - **Name**: Filename
      **Value**: {{ $json.originalFilename }} 
    - **Name**: lastModifyingUser 
      Value: {{ $json.lastModifyingUser.displayName }}

This node reads the file content, splits it into smaller logical sections (chunks), and appends relevant metadata for later retrieval.

**Example Output** from my `socimp-active-directory-organization-units.csv`:
```json
[
  {
    "response": [
      {
        "pageContent": "name: SOCIMP Groups\ndistinguishedName: OU=SOCIMP Groups,DC=socimp,DC=local",
        "metadata": {
          "source": "blob",
          "blobType": "text/csv",
          "line": 1,
          "loc": {
            "lines": {
              "from": 1,
              "to": 2
            }
          },
          "Filename": "socimp-active-directory-organization-units.csv",
          "lastMofifyingUser": "Phạm Thành Sang"
        }
      },
      {
        "pageContent": "name: SOCIMP Users\ndistinguishedName: OU=SOCIMP Users,DC=socimp,DC=local",
        "metadata": {
          "source": "blob",
          "blobType": "text/csv",
          "line": 2,
          "loc": {
            "lines": {
              "from": 1,
              "to": 2
            }
          },
          "Filename": "socimp-active-directory-organization-units.csv",
          "lastMofifyingUser": "Phạm Thành Sang"
```

>Note: The **Text Splitter** config is important for large documents. You may need to adjust the chunk size and overlap to maintain semantic integrity during splitting.

For embeddings, I chose **Google Gemini** (specifically the `text-embedding-004 model`), which offers a free usage tier and integrates well with my existing use of Google Drive. This model outputs 768-dimensional vectors.

**Example Output**:
```json

[
  { 
    "response":[
      [
        0.042045858,
        0.05958133,
        -0.06713956,
        0.010046062,
        0.009314298,
        0.024299951,
        0.007411253,
        0.0108615635,
        0.018496806,
        0.034505174,
        0.016048066,
        0.065319546,
```

The final step involves inserting these vectors into the **Pinecone database** using the `Pinecone Vector Store` node, this completes the vectorization process, storing structured, queryable internal knowledge for future LLM use.

**`Pinecone Vector Store` Node Configuration**:
- **Credential to connect with**: Conenct to Pinecone account API
- **Operantion Mode**: Insert Documents
- **Pinecone Index**
  - **From list**: socimp-n8n-index (My vector database index to store SOCIMP knowledegebase)
- **Embedding Batch Size**: 200
- **Options**
  - **Pinecone Namespace**: SOCIMP-n8n (This is optional but recommened to control the index better)

**Example Output**:
```json
[
  {
    "metadata": {
      "source": "blob",
      "blobType": "text/csv",
      "line": 1,
      "loc": {
        "lines": {
          "from": 1,
          "to": 2
        }
      },
      "Filename": "socimp-active-directory-organization-units.csv",
      "lastMofifyingUser": "Phạm Thành Sang"
    },
    "pageContent": "name: SOCIMP Groups\ndistinguishedName: OU=SOCIMP Groups,DC=socimp,DC=local"
  },
...
```

## Workflow in Actions

Triggering this workflow is simple, just upload or update a file in the specified **Google Drive folder**. Once this happens, the trigger node captures all metadata associated with the new or updated file.

![pic8](/images/n8n/pic8.png)

The rest of the workflow then proceeds to process the file, embed its contents, and store the information in the vector database, as shown below

![pic9](/images/n8n/pic9.png)

In the example above, I’ve uploaded several files into the folder. You can see that **`RECORD COUNT: 329`** entries were successfully stored in the Pinecone vector index under the namespace **`SOCIMP-n8n`**.

## Future Improment
My current focus is on making this workflow fully automated and improving how documents are processed, both in terms of efficiency and security.

Since I’m still relatively new to working with LLMs and machine learning, my priority is to deepen my understanding of the fundamentals. Once I’m more confident with the core concepts, I plan to expand and optimize this workflow further.

